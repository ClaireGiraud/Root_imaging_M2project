{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3961a2",
   "metadata": {},
   "source": [
    "# 3D U-net training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dc573",
   "metadata": {},
   "source": [
    "This script is taken from https://keras.io/examples/vision/oxford_pets_image_segmentation/\n",
    "We are doing image segmentation using a 3D U-net.\n",
    "This script contains both the training part and the test part on different data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba5e37",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages ############################\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afaf6f2",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b11525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootObject(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building \n",
    "# This is a U-net like architecture with a few differences : \n",
    "# like batch normalization, Separable convolution and dropout\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)\n",
    "    \n",
    "def EraseFile(repertoire):\n",
    "    \"\"\"Erase the precedent pictures in the folder.\"\"\"\n",
    "    import os\n",
    "    files=os.listdir(repertoire)\n",
    "    for i in range(0,len(files)):\n",
    "        os.remove(repertoire+'/'+files[i])\n",
    "\n",
    "def save_mask_pred(save_path, i):\n",
    "    \"\"\"Save the prediction\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    path = save_path + 'pred_mask_' + str(i) +'.png'\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9c335",
   "metadata": {},
   "source": [
    "#### Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part should be modified according to the dataset.\n",
    "\n",
    "# Black roots label with white background\n",
    "input_dir = \"00.Datasets/modified/blackroots/Photo/\"\n",
    "target_dir = \"00.Datasets/modified/blackroots/Masque/\"\n",
    "\n",
    "# #White roots label with black background\n",
    "#input_dir = \"00.Datasets/modified/whiteroots/Photo/\"\n",
    "#target_dir = \"00.Datasets/modified/whiteroots/Masque/\"\n",
    "\n",
    "# #Beige roots label with white background\n",
    "#input_dir = \"00.Datasets/modified/cremeroots/Photo/\"\n",
    "#target_dir = \"00.Datasets/modified/cremeroots/Masque/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e468b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parameters to define\n",
    "\n",
    "img_size = (720, 720) # input image size for U-net\n",
    "num_classes = 2 # desired number of classes for the output mask\n",
    "batch_size = 12 #Can be modified should not be more than the number of images in the differents data sets (train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset \n",
    "# \"sorted\" is important : it ensure that the photo and the label match\n",
    "\n",
    "# Photos\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Labels\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e230bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = 12 # number of validation images\n",
    "\n",
    "# Shuffle to prevent exploading gradient issue and over-fitting\n",
    "random.Random(60).shuffle(input_img_paths)\n",
    "random.Random(60).shuffle(target_img_paths)\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = RootObject(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
    "\n",
    "# #Check\n",
    "#print(train_gen)\n",
    "\n",
    "val_gen = RootObject(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different parameters can be used here\n",
    "# We changed : optimizer, loss function and number of epochs \n",
    "\n",
    "opti=keras.optimizers.RMSprop(\n",
    "    learning_rate=0.00001,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    name=\"RMSprop\")\n",
    "\n",
    "#opti2=tf.keras.optimizers.SGD(learning_rate=0.000001, momentum=0.0, nesterov=False, name='SGD')\n",
    "#opti3= tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "\n",
    "model.compile(optimizer=opti,\n",
    "              loss=\"binary_crossentropy\",\n",
    "              #loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics='Accuracy')\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"root_segmentation.h5\", save_best_only=True)]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_gen,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dc74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss and accuracy evolution during training\n",
    "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "ax_acc.plot(history.epoch, history.history[\"accuracy\"], label=\"Train accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9348fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for images in the validation set\n",
    "val_preds = model.predict(val_gen)\n",
    "\n",
    "# Display results for validation image\n",
    "i = 11\n",
    "\n",
    "# Display input image\n",
    "display(Image(filename=val_input_img_paths[i]))\n",
    "\n",
    "# Display ground-truth target mask\n",
    "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7511129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results - the predictions\n",
    "# path need to be changed\n",
    "\n",
    "path_pred_black = '03.3D_U-net/blackroots_pred/pred/'\n",
    "#path_pred_white = '03.3D_U-net/whiteroots_pred/pred/'\n",
    "#path_pred_creme = '03.3D_U-net/cremeroots_pred/pred/'\n",
    "\n",
    "EraseFile(path_pred_black)\n",
    "#EraseFile(path_pred_white)\n",
    "#EraseFile('path_pred_creme')\n",
    "\n",
    "for i in range (0, len(val_preds)): \n",
    "        save_mask_pred(path_pred_black, i)\n",
    "\n",
    "# As we don't know the composition of the validation set we need to save the labels too\n",
    "# path need to be change\n",
    "\n",
    "EraseFile('03.3D_U-net/blackroots_pred/label/')\n",
    "#EraseFile('03.3D_U-net/whiteroots_pred/label/')\n",
    "#EraseFile('03.3D_U-net/cremeroots_pred/label/')\n",
    "\n",
    "for name in val_target_img_paths : \n",
    "    img = Image.open(name)\n",
    "    name_parts = name.split('/')\n",
    "    # in name_parts[2], we have the value \"DSC_5644.png\"\n",
    "    #image_single_name = name_parts[2].split('.')\n",
    "    # #in image_single_name[0], we have the value \"DSC_5644\"\n",
    "    path = '03.3D_U-net/blackroots_pred/label/'+ 'label_' + name_parts[2]\n",
    "    #path = '03.3D_U-net/whiteroots_pred/label/'+ 'label_' + name_parts[2]\n",
    "    #path = '03.3D_U-net/cremeroots_pred/label/'+ 'label_' + name_parts[2]\n",
    "    img.save(path, 'png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
